# perseus: a fail-slow detection framework for cloud storage systems

### 摘要

最近新出现的 fail-slow 问题困扰着硬件和软件系统，其特点是受影响的组件虽然仍然在运行，但性能已经明显下降。为了解决这个问题，文本提出 perseus，一个实践中的‘fail-slow’存储设备探测框架。perseus创新性的采用了基于轻量级回归分析的模型，能够在驱动器粒度上快速定位并分析慢速故障。在 10 个月的时间内，perseus监控了 24 万块盘，发现了 304 个 fail-slow 的问题。隔离这些设备可以把节点级别的4 个 9 尾延时响应时间减少减少 48%。我们基于生产环境数据构建了大规模慢数据故障集（包含 4 万个正常盘和 315 个验证的 fail-slow 盘），并据此对慢故障盘展开了根因分析，涵盖调度算法缺陷、硬件设计瑕疵及环境因素等多类成因。我们已经公开了这个数据集。

### 1 介绍

大规模的存储系统容易受到各种故障的影响。学术界和工业界已在故障识别，检测和修复领域取得显著进展，涵盖停机故障，局部故障以及拜占庭故障等多种类型。

近来，fail-slow故障，也被称为 gray-failure 或者 limpware，正受到越来越多的关注。在 fail-slow故障里，软件或硬件组件的实际性能会低于预期。随着硬件性能的提升（Optane SSD，ZNS SSD）和软件栈（用户态驱动）的优化，fail-slow故障正在变得越来越容易检测和识别。最近的研究表明，fail-slow故障的发生率和 fail-stop 的几率相当。

准确的探测fail-slow故障是很有挑战的。性能差异有可能是内部因素引起的（SSD盘内的 GC）或者是外部的因素（负载的突发）都会有跟 fail-slow故障相似的症状。和 fail-stop 故障有明确的标准（软件 crash，数据丢失）不同，确定 fail-slow故障一般都靠经验。此外，fail-slow往往都是暂时的，很难让工程师去分辨，更不用说复现和解析根因了。

尽管已经有人在探测 fail-slow故障上做了一些工作，这些工作在大范围的云环境下就显得比较不够实用和比较低效。首先，这些方法需要访问源代码或者需要软件适配，但云厂商并不会碰用户代码。即使是内部的基础设施，插入一些代码片段也是相当耗费时间的，这些系统有可能跑了非常不同软件栈的服务。其次，现有的这些技术只能探测到节点级别，仍然需要人工才能定位到最后的原因。





